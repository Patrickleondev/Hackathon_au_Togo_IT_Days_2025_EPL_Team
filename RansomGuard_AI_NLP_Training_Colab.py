"""
Script Google Colab pour entra√Æner les mod√®les NLP
RansomGuard AI - Hackathon Togo IT Days 2025

Instructions d'utilisation:
1. Copiez ce script dans un notebook Google Colab
2. Ex√©cutez les cellules une par une
3. T√©l√©chargez les mod√®les entra√Æn√©s
4. Placez-les dans votre projet local
"""

import os
import json
import torch
import numpy as np
from datetime import datetime
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ColabNLPTrainer:
    def __init__(self):
        self.models_dir = "models/"
        os.makedirs(self.models_dir, exist_ok=True)
        
    def generate_training_data(self):
        """G√©n√©rer des donn√©es d'entra√Ænement pour NLP"""
        logger.info("üîÑ G√©n√©ration des donn√©es d'entra√Ænement...")
        
        # Donn√©es malveillantes
        malicious_texts = [
            "encrypt files ransom payment bitcoin",
            "delete system32 critical files",
            "inject malicious code process memory",
            "bypass antivirus detection sandbox",
            "steal credentials password hash",
            "create backdoor remote access",
            "modify registry keys startup",
            "disable firewall security services",
            "spread network worm virus",
            "exfiltrate sensitive data",
            "cryptojacking mining malware",
            "keylogger capture keystrokes",
            "screen capture spyware",
            "network scanning port scan",
            "privilege escalation admin",
            "persistence startup folder",
            "code obfuscation encryption",
            "anti-debugging techniques",
            "virtual machine detection",
            "timing attacks evasion",
            "ransomware encrypt files",
            "malware infection system",
            "trojan horse backdoor",
            "spyware surveillance tracking",
            "rootkit system compromise",
            "botnet command control",
            "phishing attack deception",
            "social engineering manipulation",
            "zero day exploit vulnerability",
            "advanced persistent threat"
        ]
        
        # Donn√©es l√©gitimes
        legitimate_texts = [
            "user interface design system",
            "database connection query",
            "web application framework",
            "mobile app development",
            "cloud computing services",
            "data analysis statistics",
            "machine learning algorithm",
            "network configuration setup",
            "file management system",
            "security authentication login",
            "backup restore procedure",
            "system maintenance update",
            "performance optimization",
            "error handling exception",
            "documentation user guide",
            "testing quality assurance",
            "deployment production environment",
            "monitoring logging system",
            "scalability architecture design",
            "compliance regulatory requirements",
            "software development lifecycle",
            "version control system",
            "continuous integration deployment",
            "agile methodology scrum",
            "project management planning",
            "team collaboration tools",
            "code review process",
            "unit testing framework",
            "integration testing strategy",
            "security best practices"
        ]
        
        # Combiner et √©tiqueter
        texts = malicious_texts + legitimate_texts
        labels = [1] * len(malicious_texts) + [0] * len(legitimate_texts)
        
        logger.info(f"‚úÖ {len(texts)} √©chantillons g√©n√©r√©s ({len(malicious_texts)} malveillants, {len(legitimate_texts)} l√©gitimes)")
        
        return texts, labels
    
    def train_distilbert(self, texts, labels):
        """Entra√Æner DistilBERT"""
        try:
            logger.info("üîÑ Entra√Ænement de DistilBERT...")
            
            from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
            from torch.utils.data import DataLoader, TensorDataset
            
            # Charger le mod√®le et tokenizer
            model_name = "distilbert-base-uncased"
            tokenizer = DistilBertTokenizer.from_pretrained(model_name)
            model = DistilBertForSequenceClassification.from_pretrained(
                model_name, 
                num_labels=2,
                problem_type="single_label_classification"
            )
            
            # Tokeniser les donn√©es
            encoded = tokenizer(
                texts,
                padding='max_length',
                truncation=True,
                max_length=128,
                return_tensors='pt'
            )
            
            # Cr√©er dataset
            dataset = TensorDataset(
                encoded['input_ids'],
                encoded['attention_mask'],
                torch.tensor(labels, dtype=torch.long)
            )
            
            dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
            
            # Optimiseur
            optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
            
            # Entra√Ænement
            model.train()
            for epoch in range(3):
                total_loss = 0
                for batch in dataloader:
                    input_ids, attention_mask, labels_batch = batch
                    
                    optimizer.zero_grad()
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels_batch)
                    loss = outputs.loss
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                
                avg_loss = total_loss / len(dataloader)
                logger.info(f"üìä √âpoque {epoch+1}/3 - Loss: {avg_loss:.4f}")
            
            # Sauvegarder
            save_path = f"{self.models_dir}/distilbert_hackathon"
            model.save_pretrained(save_path)
            tokenizer.save_pretrained(save_path)
            
            logger.info(f"‚úÖ DistilBERT entra√Æn√© et sauvegard√© dans {save_path}")
            return save_path
            
        except Exception as e:
            logger.error(f"‚ùå Erreur DistilBERT: {e}")
            return None
    
    def train_roberta(self, texts, labels):
        """Entra√Æner RoBERTa"""
        try:
            logger.info("üîÑ Entra√Ænement de RoBERTa...")
            
            from transformers import RobertaTokenizer, RobertaForSequenceClassification
            from torch.utils.data import DataLoader, TensorDataset
            
            # Charger le mod√®le et tokenizer
            model_name = "roberta-base"
            tokenizer = RobertaTokenizer.from_pretrained(model_name)
            model = RobertaForSequenceClassification.from_pretrained(
                model_name, 
                num_labels=2,
                problem_type="single_label_classification"
            )
            
            # Tokeniser les donn√©es
            encoded = tokenizer(
                texts,
                padding='max_length',
                truncation=True,
                max_length=128,
                return_tensors='pt'
            )
            
            # Cr√©er dataset
            dataset = TensorDataset(
                encoded['input_ids'],
                encoded['attention_mask'],
                torch.tensor(labels, dtype=torch.long)
            )
            
            dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
            
            # Optimiseur
            optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
            
            # Entra√Ænement
            model.train()
            for epoch in range(3):
                total_loss = 0
                for batch in dataloader:
                    input_ids, attention_mask, labels_batch = batch
                    
                    optimizer.zero_grad()
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels_batch)
                    loss = outputs.loss
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                
                avg_loss = total_loss / len(dataloader)
                logger.info(f"üìä √âpoque {epoch+1}/3 - Loss: {avg_loss:.4f}")
            
            # Sauvegarder
            save_path = f"{self.models_dir}/roberta_hackathon"
            model.save_pretrained(save_path)
            tokenizer.save_pretrained(save_path)
            
            logger.info(f"‚úÖ RoBERTa entra√Æn√© et sauvegard√© dans {save_path}")
            return save_path
            
        except Exception as e:
            logger.error(f"‚ùå Erreur RoBERTa: {e}")
            return None
    
    def train_codebert(self, texts, labels):
        """Entra√Æner CodeBERT"""
        try:
            logger.info("üîÑ Entra√Ænement de CodeBERT...")
            
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            from torch.utils.data import DataLoader, TensorDataset
            
            # Charger le mod√®le et tokenizer
            model_name = "microsoft/codebert-base"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(
                model_name, 
                num_labels=2,
                problem_type="single_label_classification"
            )
            
            # G√©rer les tokens sp√©ciaux
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                model.config.pad_token_id = tokenizer.eos_token_id
            
            # Tokeniser les donn√©es
            encoded = tokenizer(
                texts,
                padding='max_length',
                truncation=True,
                max_length=128,
                return_tensors='pt'
            )
            
            # Cr√©er dataset
            dataset = TensorDataset(
                encoded['input_ids'],
                encoded['attention_mask'],
                torch.tensor(labels, dtype=torch.long)
            )
            
            dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
            
            # Optimiseur
            optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
            
            # Entra√Ænement
            model.train()
            for epoch in range(3):
                total_loss = 0
                for batch in dataloader:
                    input_ids, attention_mask, labels_batch = batch
                    
                    optimizer.zero_grad()
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels_batch)
                    loss = outputs.loss
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                
                avg_loss = total_loss / len(dataloader)
                logger.info(f"üìä √âpoque {epoch+1}/3 - Loss: {avg_loss:.4f}")
            
            # Sauvegarder
            save_path = f"{self.models_dir}/codebert_hackathon"
            model.save_pretrained(save_path)
            tokenizer.save_pretrained(save_path)
            
            logger.info(f"‚úÖ CodeBERT entra√Æn√© et sauvegard√© dans {save_path}")
            return save_path
            
        except Exception as e:
            logger.error(f"‚ùå Erreur CodeBERT: {e}")
            return None
    
    def create_model_info(self, trained_models):
        """Cr√©er les informations des mod√®les"""
        model_info = {
            'training_date': datetime.now().isoformat(),
            'models': {},
            'metadata': {
                'hackathon': 'Togo IT Days 2025',
                'system': 'RansomGuard AI',
                'version': '1.0.0'
            }
        }
        
        for model_name, path in trained_models.items():
            if path:
                model_info['models'][model_name] = {
                    'path': path,
                    'status': 'trained',
                    'type': 'transformer',
                    'description': f'Mod√®le {model_name} entra√Æn√© pour la d√©tection de malware'
                }
        
        # Sauvegarder les informations
        with open(f"{self.models_dir}/model_info.json", 'w') as f:
            json.dump(model_info, f, indent=2)
        
        return model_info
    
    def run_training(self):
        """Ex√©cuter l'entra√Ænement complet"""
        logger.info("üöÄ D√©marrage de l'entra√Ænement NLP sur Google Colab...")
        
        # G√©n√©rer les donn√©es
        texts, labels = self.generate_training_data()
        
        # Entra√Æner les mod√®les
        trained_models = {}
        
        # DistilBERT
        distilbert_path = self.train_distilbert(texts, labels)
        trained_models['distilbert'] = distilbert_path
        
        # RoBERTa
        roberta_path = self.train_roberta(texts, labels)
        trained_models['roberta'] = roberta_path
        
        # CodeBERT
        codebert_path = self.train_codebert(texts, labels)
        trained_models['codebert'] = codebert_path
        
        # Cr√©er les informations des mod√®les
        model_info = self.create_model_info(trained_models)
        
        # R√©sum√©
        logger.info("üéâ Entra√Ænement termin√©!")
        logger.info(f"üìÅ Mod√®les sauvegard√©s dans: {self.models_dir}")
        logger.info(f"üìä Mod√®les entra√Æn√©s: {len([m for m in trained_models.values() if m])}")
        
        return model_info

def main():
    """Fonction principale"""
    trainer = ColabNLPTrainer()
    results = trainer.run_training()
    
    print("\n" + "="*50)
    print("üéØ R√âSULTATS DE L'ENTRA√éNEMENT")
    print("="*50)
    print(f"üìÖ Date: {results['training_date']}")
    print(f"üèÜ Hackathon: {results['metadata']['hackathon']}")
    print(f"ü§ñ Syst√®me: {results['metadata']['system']}")
    print(f"üìä Mod√®les entra√Æn√©s: {len(results['models'])}")
    
    for model_name, info in results['models'].items():
        print(f"‚úÖ {model_name.upper()}: {info['status']}")
    
    print("\nüìÅ Pour t√©l√©charger les mod√®les:")
    print("1. Allez dans le panneau de fichiers de Colab")
    print("2. Naviguez vers le dossier 'models'")
    print("3. T√©l√©chargez les dossiers des mod√®les")
    print("4. Placez-les dans votre projet local")

if __name__ == "__main__":
    main()
